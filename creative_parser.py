import requests
from bs4 import BeautifulSoup
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import time

# === –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è Google Sheets ===
SCOPE = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
CREDS = ServiceAccountCredentials.from_json_keyfile_name(
    "creativeparser-455918-93ecc38ccd1c.json", SCOPE
)
client = gspread.authorize(CREDS)
sheet = client.open("CreativeParser").sheet1

# –ü–æ–ª—É—á–∞–µ–º —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–µ–π
existing_links = [row[1] for row in sheet.get_all_values()[1:]]

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ ===
BASE_URL = "https://www.adsoftheworld.com"
HEADERS = {"User-Agent": "Mozilla/5.0"}
TARGET_COUNT = 50
parsed_count = 0
page = 0
new_cases = []

# === –ü–∞—Ä—Å–∏–º –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º, –ø–æ–∫–∞ –Ω–µ —Å–æ–±–µ—Ä—ë–º –Ω—É–∂–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ ===
while parsed_count < TARGET_COUNT:
    page_url = f"{BASE_URL}/latest?page={page}"
    print(f"–ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É: {page_url}")
    response = requests.get(page_url, headers=HEADERS)
    soup = BeautifulSoup(response.text, "html.parser")
    cards = soup.select(".views-row")

    if not cards:
        print("–ë–æ–ª—å—à–µ –Ω–µ—Ç –∫–µ–π—Å–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ.")
        break

    for card in cards:
        if parsed_count >= TARGET_COUNT:
            break

        title_tag = card.select_one(".title a")
        if not title_tag:
            continue

        title = title_tag.text.strip()
        link = BASE_URL + title_tag['href']

        if link in existing_links:
            print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ (–¥—É–±–ª–∏–∫–∞—Ç): {title}")
            continue

        # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–µ–π—Å–∞
        print(f"–ü–∞—Ä—Å–∏–º –∫–µ–π—Å: {title}")
        case_page = requests.get(link, headers=HEADERS)
        case_soup = BeautifulSoup(case_page.text, "html.parser")

        description_tag = case_soup.select_one(".field--name-field-description")
        description = description_tag.text.strip() if description_tag else "–ë–µ–∑ –æ–ø–∏—Å–∞–Ω–∏—è"

        tags_el = case_soup.select(".taxonomy-term")
        tags = ", ".join([el.text.strip() for el in tags_el]) or "–ë–µ–∑ —Ç–µ–≥–æ–≤"

        category_tag = case_soup.select_one(".field--name-field-primary-category")
        category = category_tag.text.strip() if category_tag else "–ë–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"

        date_tag = case_soup.select_one("time")
        date = date_tag.text.strip() if date_tag else "N/A"

        new_cases.append([title, link, category, tags, description, date])
        parsed_count += 1
        time.sleep(1.2)  # —á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å —Å–µ—Ä–≤–µ—Ä

    page += 1

# === –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –∫–µ–π—Å—ã –≤ —Ç–∞–±–ª–∏—Ü—É ===
if new_cases:
    sheet.append_rows(new_cases, value_input_option="USER_ENTERED")
    print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(new_cases)} –Ω–æ–≤—ã—Ö –∫–µ–π—Å–æ–≤ –≤ Google Sheets")
else:
    print("üîç –ù–æ–≤—ã—Ö –∫–µ–π—Å–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –í—Å—ë —É–∂–µ –µ—Å—Ç—å.")
